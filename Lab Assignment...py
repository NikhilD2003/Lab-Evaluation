# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vk8kZM3vMtr9rZRuOmtTi7WKK-YJ5tTJ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset from GitHub
url = "https://raw.githubusercontent.com/psrana/Lab-and-Assignment-2024/main/Fuel_cell_performance_data-Full.csv"
data = pd.read_csv(url)

# Select target variable and features
target_column = 'Target4'
X = data.drop(columns=[target_column])
y = data[target_column]

# Split dataset into 70% training and 30% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42)
}

# Train and evaluate models
results = []
predictions = {}  # Store predictions for analysis

for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)
    predictions[name] = y_pred  # Save predictions

    # Evaluate performance
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Append results
    results.append({
        "Model": name,
        "Mean Squared Error": mse,
        "R2 Score": r2
    })

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Display tabular results
print("Model Performance:")
print(results_df)

# Visualization: Bar plot of MSE
plt.figure(figsize=(10, 6))
sns.barplot(x="Model", y="Mean Squared Error", data=results_df, palette="viridis")
plt.title("Model Comparison: Mean Squared Error", fontsize=16)
plt.ylabel("Mean Squared Error", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.show()

# Visualization: Bar plot of R² Score
plt.figure(figsize=(10, 6))
sns.barplot(x="Model", y="R2 Score", data=results_df, palette="coolwarm")
plt.title("Model Comparison: R² Score", fontsize=16)
plt.ylabel("R² Score", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.show()

# Scatter plot: Actual vs Predicted for the best-performing model
best_model_name = results_df.sort_values(by="R2 Score", ascending=False).iloc[0]["Model"]
y_pred_best = predictions[best_model_name]

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_best, alpha=0.6, color="b")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)
plt.title(f"Actual vs Predicted: {best_model_name}", fontsize=16)
plt.xlabel("Actual Values", fontsize=12)
plt.ylabel("Predicted Values", fontsize=12)
plt.show()

# Feature Importance: Random Forest (if applicable)
if "Random Forest" in models:
    rf_model = models["Random Forest"]
    feature_importance = rf_model.feature_importances_
    feature_names = X.columns

    plt.figure(figsize=(10, 6))
    sns.barplot(x=feature_importance, y=feature_names, palette="magma")
    plt.title("Feature Importance: Random Forest", fontsize=16)
    plt.xlabel("Importance", fontsize=12)
    plt.ylabel("Feature", fontsize=12)
    plt.show()